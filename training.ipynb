{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4AXDtBU72u2"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "laAYPdu_7wVK"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "from tensorflow import keras\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "import copy\n",
        "\n",
        "from functions.data_processing import create_set\n",
        "from functions.prediction import make_predictions, generate_pdda_preds\n",
        "from functions.visualization import posHeatmapXY, spatial_plot\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTVjqEc68NJz"
      },
      "source": [
        "# Dataset Import"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The dataset is hosted publicly at https://doi.org/10.5281/zenodo.6303665."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hG18CQ9QdekC"
      },
      "outputs": [],
      "source": [
        "datadir = '../data' # path to data folder\n",
        "modeldir = '../models' # path to model folder (optional, if you want to save models' weights)\n",
        "preds_dir = '../preds' # path to model folder (optional, if you want to save models' predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rooms = ['testbench_01', 'testbench_01_furniture_low', 'testbench_01_furniture_mid', 'testbench_01_furniture_high']\n",
        "concrete_rooms = ['testbench_01_furniture_low_concrete', 'testbench_01_furniture_mid_concrete', 'testbench_01_furniture_high_concrete']\n",
        "other_scenarios = ['testbench_01_rotated_anchors', 'testbench_01_translated_anchors']\n",
        "anchors = ['anchor1', 'anchor2', 'anchor3', 'anchor4']\n",
        "channels = ['37','38','39']\n",
        "polarities = ['V','H']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pM2Uh7didey2"
      },
      "outputs": [],
      "source": [
        "data = defaultdict(lambda: defaultdict(lambda: defaultdict (lambda: defaultdict(list))))\n",
        "anchor_data = defaultdict(lambda: defaultdict(lambda: defaultdict (lambda: defaultdict(list))))\n",
        "\n",
        "for room in rooms + concrete_rooms + other_scenarios: \n",
        "    for channel in channels:  \n",
        "        for polarity in polarities: \n",
        "             \n",
        "            tag_filename = f'{datadir}/{room}/tag_ml_export_CH{channel}_{polarity}.json'\n",
        "            tag_df = pd.read_json(tag_filename, orient='records')\n",
        "\n",
        "            anchor_filename = f'{datadir}/{room}/anchor_ml_export_CH{channel}_{polarity}.json'\n",
        "            anchor_df = pd.read_json(anchor_filename, orient='records')\n",
        "\n",
        "            df = tag_df.merge(anchor_df)\n",
        "\n",
        "            # remove calibration points\n",
        "            df.drop(df[(df['x_tag']==0).values | (df['y_tag']==0).values | (df['z_tag']==0).values].index, inplace=True)\n",
        "            \n",
        "            for anchor in anchors:\n",
        "                data[room][anchor][channel][polarity] = df[df['anchor']==int(anchor[-1])]\n",
        "                anchor_data[room][anchor][channel][polarity] = anchor_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPOYW7bmi6e-"
      },
      "source": [
        "# Data Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The selected number of training and validation points is small so the point selection is done in an orderly fashion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OctEd1kHhcFx"
      },
      "outputs": [],
      "source": [
        "points = data['testbench_01']['anchor1']['37']['H'].iloc[:, 1:7]\n",
        "\n",
        "# only point locations that appear in all simulated environments are used.\n",
        "# some point locations that fall ontop of furniture are thus thrown away.\n",
        "for room in rooms + concrete_rooms:\n",
        "    for anchor in anchors:\n",
        "        for channel in channels:\n",
        "            for polarization in ['H','V']:\n",
        "                points = pd.merge(points, data[room][anchor][channel][polarization]['point'], on='point')\n",
        "\n",
        "# grid of training points\n",
        "xs = sorted(np.unique(points['x_tag']))[::6]\n",
        "ys = sorted(np.unique(points['y_tag']))[::3]\n",
        "train_points = points[points['x_tag'].isin(xs) & points['y_tag'].isin(ys)]\n",
        "\n",
        "# grid of validation points\n",
        "xs = sorted(np.unique(points['x_tag']))[3::10]\n",
        "ys = sorted(np.unique(points['y_tag']))[3::10]\n",
        "val_points = points[points['x_tag'].isin(xs) & points['y_tag'].isin(ys)]\n",
        "\n",
        "test_points = points.drop(index=train_points.index).drop(index=val_points.index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GVhYwlXKwBER"
      },
      "outputs": [],
      "source": [
        "print(f'Training Set Size:\\t{len(train_points)}')\n",
        "print(f'Validation Set Size:\\t{len(val_points)}')\n",
        "print(f'Testing Set Size:\\t{len(test_points)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CXmD-QbIhedi"
      },
      "outputs": [],
      "source": [
        "plt.scatter(train_points.iloc[:,1:2].values,train_points.iloc[:,2:3].values)\n",
        "plt.scatter(val_points.iloc[:,1:2].values,val_points.iloc[:,2:3].values)\n",
        "plt.legend(['Training Points', 'Validation Points'], framealpha=0.94, fancybox=True)\n",
        "ax = plt.gca()\n",
        "ax.set_xticklabels(range(-2,15,2))\n",
        "ax.set_yticklabels(range(-1,7,1))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Split the dataset based on the picked training, validation and testing points. The create_set function processes the IQ and RSSI data by applying IQ phase shifting and RSSI normalization. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The training set is augmented by reducing amplitude of IQ values and RSSI for randomly picked anchors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vdyqp2I8tBLb"
      },
      "outputs": [],
      "source": [
        "x_train, y_train = create_set(data, rooms, train_points, augmentation=True)\n",
        "x_val, y_val = create_set(data, rooms + concrete_rooms + other_scenarios, val_points)                         \n",
        "x_test, y_test = create_set(data, rooms + concrete_rooms + other_scenarios, test_points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wy1MUv38cYw"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from functions.models import jointArch\n",
        "model_arch = jointArch\n",
        "model_arch_name = 'joint_arch'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XSw4n9rPE4OP"
      },
      "outputs": [],
      "source": [
        "fit_params = {'batch_size': 128, 'validation_batch_size':32, 'epochs': 1500, 'verbose': 1, \n",
        "              'callbacks': [keras.callbacks.EarlyStopping(monitor='val_mae', mode='min', verbose=0, patience=75, restore_best_weights=True)]}\n",
        "learning_rate = 0.002"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-7uvysvOejVN"
      },
      "outputs": [],
      "source": [
        "load = False # load an already saved model\n",
        "\n",
        "if load:\n",
        "    models_dict = {}\n",
        "    for room in rooms:\n",
        "        models_dict[room] = keras.load_model(f'{modeldir}/{model_arch_name}/{room}')\n",
        "\n",
        "else:\n",
        "\n",
        "    models_dict = defaultdict(lambda: model_arch(learning_rate).model)\n",
        "\n",
        "    for training_room in rooms:\n",
        "\n",
        "        print(training_room)\n",
        "        models_dict[training_room] = model_arch(learning_rate).model\n",
        "\n",
        "        ytrain = pd.concat([y_train[training_room][anchor]['37'] for anchor in anchors], axis=1)\n",
        "        yval = pd.concat([y_val[training_room][anchor]['37'] for anchor in anchors], axis=1)\n",
        "\n",
        "        models_dict[training_room].fit(x_train[training_room], ytrain,\n",
        "                                    validation_data=(x_val[training_room], yval), \n",
        "                                    **fit_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aiuSUMZtIjGz"
      },
      "outputs": [],
      "source": [
        "save =  False # save the model\n",
        "\n",
        "if save:\n",
        "    for room in rooms:\n",
        "        models_dict[room].save(f'{modeldir}/{model_arch_name}/{room}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZfsfeDdqd2P"
      },
      "source": [
        "# Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LZxMe22Fwt-k"
      },
      "outputs": [],
      "source": [
        "# generate AoA and position predictions as well as mean euclidean distance error \n",
        "# and AoA mean absolute error for all training and testing room combinations\n",
        "preds, true_pos = make_predictions(x_test, y_test, models_dict, training_rooms=rooms, \n",
        "                                        testing_rooms=rooms + concrete_rooms + other_scenarios,\n",
        "                                        test_points=test_points, anchor_data=anchor_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def default_to_regular(d):\n",
        "    if isinstance(d, (defaultdict, dict)):\n",
        "        d = {k: default_to_regular(v) for k, v in d.items()}\n",
        "    return d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wtfSfsYs_-pw"
      },
      "outputs": [],
      "source": [
        "# save the predictions dictionary\n",
        "save_preds = False\n",
        "if save_preds:\n",
        "    results_path = f'{preds_dir}/preds_{model_arch_name}.npy'\n",
        "    np.save(results_path, default_to_regular(preds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TnVo0ChQjfRd"
      },
      "outputs": [],
      "source": [
        "# load the predictions dictionary\n",
        "load_preds = False\n",
        "if load_preds:\n",
        "    preds = np.load(f'{preds_dir}/preds_{model_arch_name}.npy', allow_pickle=True)[()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## PDDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pdda_res = generate_pdda_preds(data, rooms + concrete_rooms + other_scenarios, test_points, anchor_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WDyvvHyoyyt"
      },
      "source": [
        "# Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "posHeatmapXY(preds['pos_maes'][:,:7], pdda_res['pos_maes'][:7])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "spatial_plot(preds['pos_preds']['testbench_01_furniture_low']['testbench_01_furniture_high'], true_pos, testing_room = 'testbench_01_furniture_high', mode = 'xy',vmax = 3, cmap = 'PuBu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "bPOYW7bmi6e-"
      ],
      "name": "Triplets of Anchors Model.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
