{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4AXDtBU72u2"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "laAYPdu_7wVK"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from itertools import combinations\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, Input, callbacks\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "import copy\n",
        "\n",
        "from functions import  create_set, decreasing_signal, make_predictions, posHeatmapXY, default_to_regular, spatial_plot, generate_pdda_preds, lsq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTVjqEc68NJz"
      },
      "source": [
        "# Dataset Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hG18CQ9QdekC"
      },
      "outputs": [],
      "source": [
        "wd = '..' # path to data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rooms = ['testbench_01', 'testbench_01_furniture_low', 'testbench_01_furniture_mid', 'testbench_01_furniture_high']\n",
        "concrete_rooms = ['testbench_01_furniture_low_concrete', 'testbench_01_furniture_mid_concrete', 'testbench_01_furniture_high_concrete']\n",
        "other_scenarios = ['testbench_01_scenario2', 'testbench_01_scenario3']\n",
        "anchors = ['anchor1', 'anchor2', 'anchor3', 'anchor4']\n",
        "channels = ['37','38','39']\n",
        "polarities = ['V','H']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pM2Uh7didey2"
      },
      "outputs": [],
      "source": [
        "data = defaultdict(lambda: defaultdict(lambda: defaultdict (lambda: defaultdict(list))))\n",
        "anchor_data = defaultdict(lambda: defaultdict(lambda: defaultdict (lambda: defaultdict(list))))\n",
        "for room in rooms + concrete_rooms + other_scenarios: \n",
        "    for channel in channels:  \n",
        "        for polarity in polarities: \n",
        "             \n",
        "            tag_filename = f'{wd}/data/{room}/tag_ml_export_CH{channel}_{polarity}.json'\n",
        "            tag_df = pd.read_json(tag_filename, orient='records')\n",
        "\n",
        "            anchor_filename = f'{wd}/data/{room}/anchor_ml_export_CH{channel}_{polarity}.json'\n",
        "            anchor_df = pd.read_json(anchor_filename, orient='records')\n",
        "\n",
        "            df = tag_df.merge(anchor_df)\n",
        "            df.drop(df[(df['x_tag']==0).values | (df['y_tag']==0).values | (df['z_tag']==0).values].index, inplace=True)\n",
        "            for anchor in anchors:\n",
        "                data[room][anchor][channel][polarity] = df[df['anchor']==int(anchor[-1])]\n",
        "                anchor_data[room][anchor][channel][polarity] = anchor_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPOYW7bmi6e-"
      },
      "source": [
        "# Data Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OctEd1kHhcFx"
      },
      "outputs": [],
      "source": [
        "#Point sets creation\n",
        "uniform_points = True\n",
        "if uniform_points:\n",
        "\n",
        "    points = data['testbench_01']['anchor1']['37']['H'].iloc[:, 1:7]\n",
        "\n",
        "    # We use only the tag locations common in all rooms\n",
        "    for room in rooms + concrete_rooms:\n",
        "        for anchor in anchors:\n",
        "            for channel in channels:\n",
        "                for polarization in ['H','V']:\n",
        "                    points = pd.merge(points, data[room][anchor][channel][polarization]['point'], on='point')\n",
        "    \n",
        "    xs = sorted(np.unique(points['x_tag']))[::6]\n",
        "    ys = sorted(np.unique(points['y_tag']))[::3]\n",
        "    train_points = points[points['x_tag'].isin(xs) & points['y_tag'].isin(ys)]\n",
        "    \n",
        "    xs = sorted(np.unique(points['x_tag']))[3::10]\n",
        "    ys = sorted(np.unique(points['y_tag']))[3::10]\n",
        "    val_points = points[points['x_tag'].isin(xs) & points['y_tag'].isin(ys)]\n",
        "\n",
        "    test_points = points.drop(index=train_points.index).drop(index=val_points.index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GVhYwlXKwBER"
      },
      "outputs": [],
      "source": [
        "print(f'Train Points No.: {len(train_points)}')\n",
        "print(f'Val Points No.: {len(val_points)}')\n",
        "print(f'Test Points No.: {len(test_points)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CXmD-QbIhedi"
      },
      "outputs": [],
      "source": [
        "plt.scatter(train_points.iloc[:,1:2].values,train_points.iloc[:,2:3].values)\n",
        "plt.title('Training points')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJY7Z0Uei6fD"
      },
      "source": [
        "We have 5 IQ measurements for each anchor-tag pair, one for each antenna of the anchor. We use the angle of the IQ-value of the first antenna element as reference for the rest of the antnnas. We also use the calculated RSSI value (signal strength) as feature for each anchor-tag pair."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KDthvQDi6fD"
      },
      "source": [
        "The following are datasets that only use the polarization with the higher signal strength (RSSI) for each point."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vdyqp2I8tBLb"
      },
      "outputs": [],
      "source": [
        "#train/val/test sets creation\n",
        "x_train, y_train = create_set(data, train_points)\n",
        "\n",
        "x_val, y_val = create_set(data, val_points)\n",
        "                              \n",
        "x_test, y_test = create_set(data, test_points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bvB8G_lTbOd6"
      },
      "outputs": [],
      "source": [
        "#data augmentation in training set\n",
        "x_reduced_signal_amp = []\n",
        "\n",
        "for i in range(30):\n",
        "    x_reduced_signal_amp.append(decreasing_signal(x_train, rooms, scale_util=5))\n",
        "\n",
        "x_train_reduced_amp = copy.deepcopy(x_train)\n",
        "y_train_reduced_amp = copy.deepcopy(y_train)\n",
        "\n",
        "for room in rooms:\n",
        "    for anchor in anchors:\n",
        "        for channel in channels:\n",
        "            for i in range(30):\n",
        "                x_train_reduced_amp[room][anchor][channel] = pd.concat([x_train_reduced_amp[room][anchor][channel], x_reduced_signal_amp[i][room][anchor][channel]])\n",
        "                \n",
        "            for i in range(30):\n",
        "                y_train_reduced_amp[room][anchor][channel] = pd.concat([y_train_reduced_amp[room][anchor][channel], y_train[room][anchor][channel]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wy1MUv38cYw"
      },
      "source": [
        "# Triplet of Anchors AoA Estimation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBh_UeKV1k3l"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BYChdONEy5sE"
      },
      "outputs": [],
      "source": [
        "def aoa_mlp(anchors, lr, arch=[[24], [0.1]]):\n",
        "\n",
        "    '''\n",
        "    Input: 3 x (8 IQ Values, 1 x RSSI, 1 x IQ Reference Values) (for three anchors)\n",
        "    Output: A latence space 9 x 1\n",
        "    '''\n",
        "\n",
        "    inp = {}\n",
        "    x = []\n",
        "    for anchor in anchors:\n",
        "        inp[anchor] = Input((10,))\n",
        "        x.append(inp[anchor])\n",
        "\n",
        "    x = layers.concatenate(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    for i in range(len(arch[0])):\n",
        "        x = layers.Dense(arch[0][i], activation='relu')(x)\n",
        "        x = layers.Dropout(arch[1][i])(x)\n",
        "\n",
        "    out = layers.Dense(9)(x)\n",
        "\n",
        "    model = tf.keras.Model(\n",
        "        inputs=inp,\n",
        "        outputs=out,\n",
        "    )\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1rv44IrTy5sF"
      },
      "outputs": [],
      "source": [
        "def smol_mlp(lr):\n",
        "\n",
        "    '''\n",
        "    Input: The outputs of the aoa_mlps concatenated\n",
        "    Output: A latence space 12x1 \n",
        "    '''\n",
        "\n",
        "    inp1 = Input((27,), name='AoAs')\n",
        "    inp2 = Input((9), name='Powers ')\n",
        "\n",
        "    x = layers.concatenate([inp1, inp2])\n",
        "\n",
        "    out = layers.Dense(27)(x)\n",
        "    out = layers.Dropout(0.15)(out)\n",
        "    out = layers.Dense(12)(out)\n",
        "    \n",
        "    model = tf.keras.Model(\n",
        "        inputs=[inp1, inp2],\n",
        "        outputs=out,\n",
        "    )\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AKM1Xd_Gy5sG"
      },
      "outputs": [],
      "source": [
        "def three_anchor_model(lr=0.01):\n",
        "    '''\n",
        "    Input: 3 x 4 x (8 IQ Values, 1 x RSSI, 1 x IQ Reference Values) (for the 4 anchors and the 3 channels)\n",
        "    Output: The 8 predicted Azimuthian and Elevation AoAs of the four anchors.\n",
        "    '''\n",
        "    combs = list(combinations(anchors,3))\n",
        "    \n",
        "    x = defaultdict(dict)\n",
        "    p = defaultdict(dict)\n",
        "    for channel in channels:\n",
        "        for anchor in anchors:\n",
        "            x[anchor][channel] = Input((10,), name=f'{anchor}-{channel}')\n",
        "            p[anchor][channel] = layers.Lambda(lambda x: x[:,1:2])(x[anchor][channel])\n",
        "\n",
        "    y = defaultdict(list)\n",
        "    w = []\n",
        "    for i,comb in enumerate(combs):\n",
        "        for channel in channels:\n",
        "            y[i].append(aoa_mlp(comb, lr=lr)([x[anchor][channel] for anchor in comb]))\n",
        "\n",
        "        y[i] = layers.concatenate(y[i])\n",
        "        powers = layers.concatenate([p[anchor][channel] for anchor in comb for channel in channels])\n",
        "        y[i] = smol_mlp(lr=lr)([y[i], powers])\n",
        "        w.append(y[i])\n",
        "        w.append(powers)\n",
        "    \n",
        "    w = layers.concatenate(w)\n",
        "    \n",
        "    w = layers.Dropout(0.1)(w)\n",
        "    w = layers.Dense(32, activation='relu')(w)\n",
        "    w = layers.Dropout(0.1)(w)\n",
        "    w = layers.Dense(16, activation='relu')(w)\n",
        "\n",
        "    out = layers.Dense(8)(w)\n",
        "\n",
        "    model = tf.keras.Model(\n",
        "        inputs=x,\n",
        "        outputs=out,\n",
        "    )\n",
        "\n",
        "    model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=lr), loss = 'mse', metrics = 'mae')\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UI0D3tBbkx5N"
      },
      "outputs": [],
      "source": [
        "three_anchor_model(0.001).summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NQIAT89ztiZt"
      },
      "outputs": [],
      "source": [
        "aoa_mlp([1,2,3],0.001).summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xXwsEvKAtwWQ"
      },
      "outputs": [],
      "source": [
        "smol_mlp(0.001).summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHL_boWT-XgK"
      },
      "source": [
        "## Optimizing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XSw4n9rPE4OP"
      },
      "outputs": [],
      "source": [
        "#Train parameters\n",
        "fit_params = {'batch_size': 128, 'validation_batch_size':32, 'epochs': 50, 'verbose': 1,\n",
        "              'callbacks': [callbacks.EarlyStopping(monitor='val_mae', mode='min', verbose=0, patience=75, restore_best_weights=True)]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-7uvysvOejVN"
      },
      "outputs": [],
      "source": [
        "#Model training\n",
        "load = False\n",
        "\n",
        "if load:\n",
        "    triplets_models = {}\n",
        "    for room in rooms:\n",
        "        triplets_models[room] = models.load_model(f'/path_to_model/triplets_model/{room}') #add your path\n",
        "\n",
        "else:\n",
        "\n",
        "    triplets_models = defaultdict(lambda: three_anchor_model(0.002))\n",
        "\n",
        "    for training_room in rooms:\n",
        "\n",
        "        print(training_room)\n",
        "        triplets_models[training_room] = three_anchor_model(0.002)\n",
        "\n",
        "        ytrain = pd.concat([y_train_reduced_amp[training_room][anchor]['37'] for anchor in anchors], axis=1)\n",
        "        yval = pd.concat([y_val[training_room][anchor]['37'] for anchor in anchors], axis=1)\n",
        "\n",
        "        triplets_models[training_room].fit(x_train_reduced_amp[training_room], ytrain,\n",
        "                                    validation_data=(x_val[training_room], yval), \n",
        "                                    **fit_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aiuSUMZtIjGz"
      },
      "outputs": [],
      "source": [
        "# You can save the model\n",
        "save =  False\n",
        "\n",
        "if save:\n",
        "    for room in rooms:\n",
        "        triplets_models[room].save(f'/path_to_model/triplets_model/{room}') #add your path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZfsfeDdqd2P"
      },
      "source": [
        "## Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LZxMe22Fwt-k"
      },
      "outputs": [],
      "source": [
        "# Make predictions\n",
        "preds, true_pos = make_predictions(x_test, y_test, triplets_models, training_rooms=rooms, testing_rooms=rooms+concrete_rooms+other_scenarios,\n",
        "                                     test_points=test_points, anchor_data = anchor_data)\n",
        "\n",
        "triplets_pos_preds = preds['pos_preds']\n",
        "triplets_pos_maes = preds['pos_maes']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wtfSfsYs_-pw"
      },
      "outputs": [],
      "source": [
        "# You can save your predictions\n",
        "save_preds = False\n",
        "if save_preds:\n",
        "    results_path = f'/path_to_preds/triplets_preds.npy' #add your path\n",
        "    np.save(results_path, default_to_regular(preds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TnVo0ChQjfRd"
      },
      "outputs": [],
      "source": [
        "# You can load your predictions\n",
        "load_preds = False\n",
        "if load_preds:\n",
        "    preds = np.load(f'/path_to_preds/triplets_preds.npy', allow_pickle=True)[()] #add your path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## PDDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pdda_res = generate_pdda_preds(data, test_points, anchor_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WDyvvHyoyyt"
      },
      "source": [
        "# Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iyWVW4Am8uAw"
      },
      "outputs": [],
      "source": [
        "posHeatmapXY(triplets_pos_maes[:,:7], pdda_res['pos_maes'][:7])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A7E9W2a2o5o-"
      },
      "outputs": [],
      "source": [
        "spatial_plot(triplets_pos_preds['testbench_01_furniture_low']['testbench_01_furniture_high'], true_pos, testing_room = 'testbench_01_furniture_high', mode = 'xy',vmax = 3, cmap = 'YlOrRd')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "bPOYW7bmi6e-"
      ],
      "name": "Triplets of Anchors Model.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
